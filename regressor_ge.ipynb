{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'PonyGE2' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/PonyGE/PonyGE2.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "!cd PonyGE2/ && git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: numpy in c:\\users\\hsabi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.24.1)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#pip installs\n",
    "\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sewar in c:\\users\\hsabi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.4.5)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy in c:\\users\\hsabi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sewar) (1.24.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\hsabi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sewar) (1.10.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\hsabi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sewar) (9.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install sewar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\hsabi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.1.1)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\hsabi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture captured\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "experiment = \"regression\"\n",
    "\n",
    "shutil.copy(f\"./fitness/{experiment}.py\", \"./PonyGE2/src/fitness/\")\n",
    "!cd ./PonyGE2/src && python ponyge.py --parameters ../../parameters/{experiment}.txt\n",
    "os.remove(f\"./PonyGE2/src/fitness/{experiment}.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sewar.full_ref import mse\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "captured_str = str(captured).split('\\n')\n",
    "for line in captured_str:\n",
    "    if \"Phenotype: \" in line:\n",
    "        phenotype = line.replace(\"Phenotype: \", \"\").strip()\n",
    "    elif \"DATASET_NAME\" in line:\n",
    "        dataset_name = line.replace(\"DATASET_NAME: \", \"\")\n",
    "\n",
    "dataset_path = './datasets/data.xlsx'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def extremeVal(reference, val):\n",
    "    if (val > 1e2*reference) or (val < -1e2*reference):\n",
    "        return 0\n",
    "    else:\n",
    "        return val\n",
    "\n",
    "split_type_index = {\"arima\": 2, \"sarima\": 1}\n",
    "models_values_column = {\n",
    "    \"arima\": {\n",
    "        \"series\": 1,\n",
    "        \"arima\": 3,\n",
    "        \"mlp\": 4,\n",
    "        \"svr\": 5,\n",
    "        \"rbf\": 6,\n",
    "    },\n",
    "    \"sarima\": {\n",
    "        \"series\": 6,\n",
    "        \"sarima\": 2,\n",
    "        \"mlp\": 3,\n",
    "        \"svr\": 4,\n",
    "        \"rbf\": 5,\n",
    "    },\n",
    "}\n",
    "window_size = 5\n",
    "\n",
    "def load_data(dataset_name, dataset_path, linear_type):\n",
    "    wb = load_workbook(dataset_path)\n",
    "    dataset_name_arr = [dataset_name]\n",
    "\n",
    "    dataset = {}\n",
    "    for name in dataset_name_arr:\n",
    "\n",
    "        ws = wb[name]\n",
    "        columns = [ws[\"A\"], ws[\"B\"], ws[\"C\"],\n",
    "                    ws[\"D\"], ws[\"E\"], ws[\"F\"], ws[\"G\"]]\n",
    "\n",
    "        sheet = [[] for _ in range(len(columns[0]))]\n",
    "\n",
    "        for c in columns:\n",
    "            for index, item in enumerate(c):\n",
    "                sheet[index].append(item.value)\n",
    "\n",
    "        sheet = np.array(sheet)\n",
    "        dataset[name] = {\"raw\": sheet}\n",
    "\n",
    "    for name, _ in dataset.items():\n",
    "        sheet = dataset[name][\"raw\"]\n",
    "\n",
    "        dataset[name][\"series\"] = {\n",
    "            \"treino\": [], \"teste\": [], \"validacao\": []}\n",
    "        dataset[name][linear_type] = {\n",
    "            \"treino\": [], \"teste\": [], \"validacao\": []}\n",
    "        dataset[name][\"mlp\"] = {\"treino\": [], \"teste\": [], \"validacao\": []}\n",
    "        dataset[name][\"svr\"] = {\"treino\": [], \"teste\": [], \"validacao\": []}\n",
    "        dataset[name][\"rbf\"] = {\"treino\": [], \"teste\": [], \"validacao\": []}\n",
    "\n",
    "        for row in sheet[1:]:\n",
    "            split_type = row[split_type_index[linear_type]]\n",
    "            if linear_type == \"arima\" and row[4] == None:\n",
    "                continue\n",
    "            key = None\n",
    "            if split_type.lower() == \"teste\":\n",
    "                key = \"teste\"\n",
    "            elif split_type.lower() == \"validacao\" or split_type.lower() == \"validação\":\n",
    "                key = \"validacao\"\n",
    "            elif split_type.lower() == \"treinamento\":\n",
    "                key = \"treino\"\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            for model_key, val in models_values_column[linear_type].items():\n",
    "                dataset[name][model_key][key].append(float(row[val]))\n",
    "\n",
    "        for type_data in [\"teste\", \"treino\", \"validacao\"]:\n",
    "            dataset[name][\"series\"][type_data] = np.array(\n",
    "                dataset[name][\"series\"][type_data])\n",
    "            dataset[name][linear_type][type_data] = np.array(\n",
    "                dataset[name][linear_type][type_data])\n",
    "            dataset[name][\"mlp\"][type_data] = np.array(\n",
    "                dataset[name][\"mlp\"][type_data])\n",
    "            dataset[name][\"svr\"][type_data] = np.array(\n",
    "                dataset[name][\"svr\"][type_data])\n",
    "            dataset[name][\"rbf\"][type_data] = np.array(\n",
    "                dataset[name][\"rbf\"][type_data])\n",
    "\n",
    "    fixExtreme = np.vectorize(extremeVal)\n",
    "\n",
    "    for name in dataset.keys():\n",
    "        reference = np.max(dataset[name][\"series\"][\"treino\"])\n",
    "\n",
    "        for type_data in [\"teste\", \"treino\", \"validacao\"]:\n",
    "            dataset[name][\"mlp\"][type_data] = fixExtreme(\n",
    "                reference, dataset[name][\"mlp\"][type_data])\n",
    "            dataset[name][\"svr\"][type_data] = fixExtreme(\n",
    "                reference, dataset[name][\"svr\"][type_data])\n",
    "            dataset[name][\"rbf\"][type_data] = fixExtreme(\n",
    "                reference, dataset[name][\"rbf\"][type_data])\n",
    "            dataset[name][linear_type][type_data] = fixExtreme(\n",
    "                reference, dataset[name][linear_type][type_data])\n",
    "\n",
    "    return dataset[dataset_name]\n",
    "\n",
    "def build_model(phenotype):\n",
    "    model = {\"linear\": {}, \"nlinear\": {}}\n",
    "\n",
    "    linear, nlinear = phenotype.split(';')\n",
    "\n",
    "    weight_tuple, linear_model = linear.split(':')\n",
    "\n",
    "    weight_values = weight_tuple.removeprefix('(').removesuffix(')')\n",
    "    model[\"linear\"][linear_model] = [\n",
    "        float(x) for x in weight_values.split(' ')]\n",
    "\n",
    "    nlinear_parts = nlinear.split(',')\n",
    "\n",
    "    for i in range(len(nlinear_parts)):\n",
    "        weight_tuple, nlinear_model = nlinear_parts[i].split(':')\n",
    "        weight_values = weight_tuple.removeprefix('(').removesuffix(')')\n",
    "\n",
    "        if nlinear_model in model[\"nlinear\"]:\n",
    "            model[\"nlinear\"][nlinear_model] += [\n",
    "                float(x) for x in weight_values.split(' ')]\n",
    "        else:\n",
    "            model[\"nlinear\"][nlinear_model] = [\n",
    "                float(x) for x in weight_values.split(' ')]\n",
    "\n",
    "    return model\n",
    "\n",
    "def predict_mse(model, arima_dataset, sarima_dataset, split):\n",
    "    if \"arima\" in model[\"linear\"].keys():\n",
    "        dataset = arima_dataset\n",
    "        linear_type = \"arima\"\n",
    "    else:\n",
    "        dataset = sarima_dataset\n",
    "        linear_type = \"sarima\"\n",
    "    predict = dataset[\"series\"][split]\n",
    "    X_train = apply_window(\n",
    "        window_size, split, linear_type, dataset, model_names=model[\"nlinear\"].keys())\n",
    "    kernel = []\n",
    "    for i in range(window_size):\n",
    "        for _, val in model[\"linear\"].items():\n",
    "            kernel.append(val[i])\n",
    "        for _, val in model[\"nlinear\"].items():\n",
    "            kernel.append(val[i])\n",
    "    return mse(create_prediction(X_train, np.reshape(kernel, (1, -1))), predict)\n",
    "\n",
    "def apply_window(window_size, data_type, linear_type, dataset, model_names=None):\n",
    "    size_pred = len(dataset[\"series\"][data_type])\n",
    "    new_data = []\n",
    "    for w in range(window_size):\n",
    "        linear = np.concatenate(\n",
    "            [np.zeros(w), dataset[linear_type][data_type]])\n",
    "        linear = linear[:size_pred]\n",
    "        row = np.array(linear)\n",
    "        for model_name in model_names:\n",
    "            nonlinear = np.concatenate(\n",
    "                [np.zeros(w), dataset[model_name][data_type]])\n",
    "            nonlinear = nonlinear[:size_pred]\n",
    "            row = np.column_stack([row, nonlinear])\n",
    "        if len(new_data) == 0:\n",
    "            new_data = row\n",
    "        else:\n",
    "            new_data = np.column_stack([new_data, row])\n",
    "    return np.array(new_data)\n",
    "\n",
    "def create_prediction(X, kernel):\n",
    "    X = torch.tensor(np.expand_dims(X, axis=(0, 1))).float()\n",
    "    kernel = torch.tensor(np.expand_dims(kernel, axis=(0, 1))).float()\n",
    "    X.to(device)\n",
    "    kernel.to(device)\n",
    "    result = F.conv2d(X, kernel)\n",
    "    return result.numpy().squeeze().astype(\"f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: B1H\n",
      "Fenótipo:   (0.0 0 1 0 0 05326067.5 88 44859.82 0922.77 647):sarima;(2 0 0 9 0.035 2 1 5859.1 15 64):rbf\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m arima_dataset \u001b[39m=\u001b[39m load_data(dataset_name, \u001b[39m'\u001b[39m\u001b[39m./datasets/arima_data.xlsx\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39marima\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m sarima_dataset \u001b[39m=\u001b[39m load_data(dataset_name, \u001b[39m'\u001b[39m\u001b[39m./datasets/sarima_data.xlsx\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msarima\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m model \u001b[39m=\u001b[39m build_model(phenotype)\n\u001b[0;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mFitness treino:\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpredict_mse(model,\u001b[39m \u001b[39marima_dataset,\u001b[39m \u001b[39msarima_dataset,\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtreino\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m:\u001b[39;00m\u001b[39m.3e\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mFitness teste:\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpredict_mse(model,\u001b[39m \u001b[39marima_dataset,\u001b[39m \u001b[39msarima_dataset,\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mteste\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m:\u001b[39;00m\u001b[39m.3e\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[32], line 115\u001b[0m, in \u001b[0;36mbuild_model\u001b[1;34m(phenotype)\u001b[0m\n\u001b[0;32m    112\u001b[0m weight_tuple, linear_model \u001b[39m=\u001b[39m linear\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    114\u001b[0m weight_values \u001b[39m=\u001b[39m weight_tuple\u001b[39m.\u001b[39mremoveprefix(\u001b[39m'\u001b[39m\u001b[39m(\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mremovesuffix(\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 115\u001b[0m model[\u001b[39m\"\u001b[39m\u001b[39mlinear\u001b[39m\u001b[39m\"\u001b[39m][linear_model] \u001b[39m=\u001b[39m [\n\u001b[0;32m    116\u001b[0m     \u001b[39mfloat\u001b[39;49m(x) \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m weight_values\u001b[39m.\u001b[39;49msplit(\u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m'\u001b[39;49m)]\n\u001b[0;32m    118\u001b[0m nlinear_parts \u001b[39m=\u001b[39m nlinear\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    120\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(nlinear_parts)):\n",
      "Cell \u001b[1;32mIn[32], line 116\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    112\u001b[0m weight_tuple, linear_model \u001b[39m=\u001b[39m linear\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    114\u001b[0m weight_values \u001b[39m=\u001b[39m weight_tuple\u001b[39m.\u001b[39mremoveprefix(\u001b[39m'\u001b[39m\u001b[39m(\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mremovesuffix(\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    115\u001b[0m model[\u001b[39m\"\u001b[39m\u001b[39mlinear\u001b[39m\u001b[39m\"\u001b[39m][linear_model] \u001b[39m=\u001b[39m [\n\u001b[1;32m--> 116\u001b[0m     \u001b[39mfloat\u001b[39;49m(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m weight_values\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)]\n\u001b[0;32m    118\u001b[0m nlinear_parts \u001b[39m=\u001b[39m nlinear\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    120\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(nlinear_parts)):\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: ''"
     ]
    }
   ],
   "source": [
    "print(f'Dataset: {dataset_name}')\n",
    "print(f'Fenótipo: {phenotype}')\n",
    "\n",
    "arima_dataset = load_data(dataset_name, './datasets/arima_data.xlsx', 'arima')\n",
    "sarima_dataset = load_data(dataset_name, './datasets/sarima_data.xlsx', 'sarima')\n",
    "model = build_model(phenotype)\n",
    "print('Fitness treino:', f\"{predict_mse(model, arima_dataset, sarima_dataset, 'treino'):.3e}\")\n",
    "print('Fitness teste:', f\"{predict_mse(model, arima_dataset, sarima_dataset, 'teste'):.3e}\")\n",
    "print('Fitness validacao:', f\"{predict_mse(model, arima_dataset, sarima_dataset, 'validacao'):.3e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec24db49983a949fb72ed67dd1ffbda33d167907641147950d83ba6b0bf33f37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
