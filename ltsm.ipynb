{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8eb0896a",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ea42af73",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Starting LSTM Experiments\n",
            "Datasets: 10\n",
            "Runs per dataset: 5\n",
            "Max epochs: 200 (patience=15)\n",
            "============================================================\n",
            "\n",
            "\n",
            "############################################################\n",
            "DATASET: CARSALES\n",
            "############################################################\n",
            "[CARSALES] Using columns - SERIE: idx=1 (SERIE), SPLIT: idx=2 (SPLIT)\n",
            "  Row 2: SERIE=0.0478301105645122, SPLIT=Treinamento\n",
            "  Row 3: SERIE=0.15391359407724903, SPLIT=Treinamento\n",
            "  Row 4: SERIE=0.3145487311869855, SPLIT=Treinamento\n",
            "[CARSALES] Data loaded: n=108, range=[0.00e+00, 1.00e+00], mean=4.40e-01\n",
            "[CARSALES] Splits: Train=52, Val=36, Test=20\n",
            "CARSALES - Run 1/5 - Test MSE: 1.224706e-01, RMSE: 3.499579e-01\n",
            "CARSALES - Run 2/5 - Test MSE: 1.374454e-01, RMSE: 3.707362e-01\n",
            "CARSALES - Run 3/5 - Test MSE: 1.559389e-01, RMSE: 3.948909e-01\n",
            "CARSALES - Run 4/5 - Test MSE: 1.262397e-01, RMSE: 3.553023e-01\n",
            "CARSALES - Run 5/5 - Test MSE: 1.390941e-01, RMSE: 3.729532e-01\n",
            "\n",
            "CARSALES Summary:\n",
            "Test MSE:  1.362377e-01 ± 1.172388e-02\n",
            "Test RMSE: 3.687681e-01 ± 1.574130e-02\n",
            "\n",
            "\n",
            "############################################################\n",
            "DATASET: Electricity\n",
            "############################################################\n",
            "[Electricity] Using columns - SERIE: idx=1 (SERIE), SPLIT: idx=2 (SPLIT)\n",
            "  Row 2: SERIE=0.05433884297520661, SPLIT=Treinamento\n",
            "  Row 3: SERIE=0.03384297520661157, SPLIT=Treinamento\n",
            "  Row 4: SERIE=0.02371900826446281, SPLIT=Treinamento\n",
            "[Electricity] Data loaded: n=486, range=[0.00e+00, 4.13e+10], mean=8.50e+07\n",
            "[Electricity] Splits: Train=252, Val=169, Test=65\n",
            "[Electricity] WARNING: Found 1 corrupted values > 10.0\n",
            "  Outlier indices: [3]\n",
            "  Outlier values: [4.1322314e+10]\n",
            "  Replaced index 3 (value=4.56e-02) with median=0.0456\n",
            "[Electricity] Fixed! New range: [0.00e+00, 1.00e+00]\n",
            "Electricity - Run 1/5 - Test MSE: 2.472940e-02, RMSE: 1.572558e-01\n",
            "Electricity - Run 2/5 - Test MSE: 2.208639e-02, RMSE: 1.486149e-01\n",
            "Electricity - Run 3/5 - Test MSE: 2.767595e-02, RMSE: 1.663609e-01\n",
            "Electricity - Run 4/5 - Test MSE: 3.182216e-02, RMSE: 1.783877e-01\n",
            "Electricity - Run 5/5 - Test MSE: 2.356117e-02, RMSE: 1.534965e-01\n",
            "\n",
            "Electricity Summary:\n",
            "Test MSE:  2.597501e-02 ± 3.452009e-03\n",
            "Test RMSE: 1.608232e-01 ± 1.053210e-02\n",
            "\n",
            "\n",
            "############################################################\n",
            "DATASET: GAS\n",
            "############################################################\n",
            "[GAS] Using columns - SERIE: idx=1 (SERIE), SPLIT: idx=2 (SPLIT)\n",
            "  Row 2: SERIE=0.004762524552145207, SPLIT=Treinamento\n",
            "  Row 3: SERIE=0.0, SPLIT=Treinamento\n",
            "  Row 4: SERIE=0.0565113472324112, SPLIT=Treinamento\n",
            "[GAS] Data loaded: n=192, range=[0.00e+00, 1.00e+00], mean=4.45e-01\n",
            "[GAS] Splits: Train=106, Val=72, Test=14\n",
            "GAS - Run 1/5 - Test MSE: 4.350649e-03, RMSE: 6.595945e-02\n",
            "GAS - Run 2/5 - Test MSE: 1.937387e-03, RMSE: 4.401576e-02\n",
            "GAS - Run 3/5 - Test MSE: 1.815596e-01, RMSE: 4.260981e-01\n",
            "GAS - Run 4/5 - Test MSE: 1.466846e-01, RMSE: 3.829942e-01\n",
            "GAS - Run 5/5 - Test MSE: 1.721216e-03, RMSE: 4.148754e-02\n",
            "\n",
            "GAS Summary:\n",
            "Test MSE:  6.725068e-02 ± 7.986566e-02\n",
            "Test RMSE: 1.921110e-01 ± 1.741954e-01\n",
            "\n",
            "\n",
            "############################################################\n",
            "DATASET: LAKEERIE\n",
            "############################################################\n",
            "[LAKEERIE] Using columns - SERIE: idx=1 (SERIE), SPLIT: idx=2 (SPLIT)\n",
            "  Row 2: SERIE=0.4763, SPLIT=Treinamento\n",
            "  Row 3: SERIE=0.4648999999999999, SPLIT=Treinamento\n",
            "  Row 4: SERIE=0.5085000000000001, SPLIT=Treinamento\n",
            "[LAKEERIE] Data loaded: n=600, range=[0.00e+00, 1.00e+00], mean=4.99e-01\n",
            "[LAKEERIE] Splits: Train=300, Val=200, Test=100\n",
            "LAKEERIE - Run 1/5 - Test MSE: 1.576219e-03, RMSE: 3.970163e-02\n",
            "LAKEERIE - Run 2/5 - Test MSE: 1.535522e-03, RMSE: 3.918573e-02\n",
            "LAKEERIE - Run 3/5 - Test MSE: 1.692743e-03, RMSE: 4.114295e-02\n",
            "LAKEERIE - Run 4/5 - Test MSE: 1.424278e-03, RMSE: 3.773961e-02\n",
            "LAKEERIE - Run 5/5 - Test MSE: 2.412960e-03, RMSE: 4.912189e-02\n",
            "\n",
            "LAKEERIE Summary:\n",
            "Test MSE:  1.728344e-03 ± 3.529150e-04\n",
            "Test RMSE: 4.137836e-02 ± 4.021871e-03\n",
            "\n",
            "\n",
            "############################################################\n",
            "DATASET: Nordic\n",
            "############################################################\n",
            "[Nordic] Using columns - SERIE: idx=1 (SERIE), SPLIT: idx=2 (SPLIT)\n",
            "  Row 2: SERIE=0.357677560708417, SPLIT=Treinamento\n",
            "  Row 3: SERIE=0.3263100237356217, SPLIT=Treinamento\n",
            "  Row 4: SERIE=0.30009129085265657, SPLIT=Treinamento\n",
            "[Nordic] Data loaded: n=3121, range=[0.00e+00, 1.00e+00], mean=5.05e-01\n",
            "[Nordic] Splits: Train=1512, Val=1009, Test=600\n",
            "Nordic - Run 1/5 - Test MSE: 7.239862e-04, RMSE: 2.690699e-02\n",
            "Nordic - Run 2/5 - Test MSE: 9.858333e-04, RMSE: 3.139798e-02\n",
            "Nordic - Run 3/5 - Test MSE: 1.238572e-03, RMSE: 3.519335e-02\n",
            "Nordic - Run 4/5 - Test MSE: 1.671543e-03, RMSE: 4.088451e-02\n",
            "Nordic - Run 5/5 - Test MSE: 7.490450e-04, RMSE: 2.736869e-02\n",
            "\n",
            "Nordic Summary:\n",
            "Test MSE:  1.073796e-03 ± 3.521422e-04\n",
            "Test RMSE: 3.235030e-02 ± 5.220512e-03\n",
            "\n",
            "\n",
            "############################################################\n",
            "DATASET: PIGS\n",
            "############################################################\n",
            "[PIGS] Using columns - SERIE: idx=1 (SERIE), SPLIT: idx=2 (SPLIT)\n",
            "  Row 2: SERIE=0.4924633013173292, SPLIT=Treinamento\n",
            "  Row 3: SERIE=0.44112569660877526, SPLIT=Treinamento\n",
            "  Row 4: SERIE=0.0, SPLIT=Treinamento\n",
            "[PIGS] Data loaded: n=188, range=[0.00e+00, 1.00e+00], mean=6.58e-01\n",
            "[PIGS] Splits: Train=91, Val=62, Test=35\n",
            "PIGS - Run 1/5 - Test MSE: 1.181603e-02, RMSE: 1.087016e-01\n",
            "PIGS - Run 2/5 - Test MSE: 1.608505e-02, RMSE: 1.268269e-01\n",
            "PIGS - Run 3/5 - Test MSE: 1.364036e-02, RMSE: 1.167920e-01\n",
            "PIGS - Run 4/5 - Test MSE: 1.557645e-02, RMSE: 1.248057e-01\n",
            "PIGS - Run 5/5 - Test MSE: 1.354332e-02, RMSE: 1.163758e-01\n",
            "\n",
            "PIGS Summary:\n",
            "Test MSE:  1.413224e-02 ± 1.539663e-03\n",
            "Test RMSE: 1.187004e-01 ± 6.516687e-03\n",
            "\n",
            "\n",
            "############################################################\n",
            "DATASET: POLLUTION\n",
            "############################################################\n",
            "[POLLUTION] Using columns - SERIE: idx=1 (SERIE), SPLIT: idx=2 (SPLIT)\n",
            "  Row 2: SERIE=0.0003217503808389559, SPLIT=Treinamento\n",
            "  Row 3: SERIE=0.0, SPLIT=Treinamento\n",
            "  Row 4: SERIE=0.008043759520973916, SPLIT=Treinamento\n",
            "[POLLUTION] Data loaded: n=130, range=[0.00e+00, 1.00e+00], mean=2.42e-01\n",
            "[POLLUTION] Splits: Train=70, Val=48, Test=12\n",
            "POLLUTION - Run 1/5 - Test MSE: 2.340035e-02, RMSE: 1.529717e-01\n",
            "POLLUTION - Run 2/5 - Test MSE: 1.451676e-02, RMSE: 1.204855e-01\n",
            "POLLUTION - Run 3/5 - Test MSE: 1.492221e-02, RMSE: 1.221565e-01\n",
            "POLLUTION - Run 4/5 - Test MSE: 2.203293e-02, RMSE: 1.484349e-01\n",
            "POLLUTION - Run 5/5 - Test MSE: 2.303088e-02, RMSE: 1.517593e-01\n",
            "\n",
            "POLLUTION Summary:\n",
            "Test MSE:  1.958063e-02 ± 3.996297e-03\n",
            "Test RMSE: 1.391616e-01 ± 1.465187e-02\n",
            "\n",
            "\n",
            "############################################################\n",
            "DATASET: REDWINE\n",
            "############################################################\n",
            "[REDWINE] Using columns - SERIE: idx=1 (SERIE), SPLIT: idx=2 (SPLIT)\n",
            "  Row 2: SERIE=0.0, SPLIT=Treinamento\n",
            "  Row 3: SERIE=0.061000289100896214, SPLIT=Treinamento\n",
            "  Row 4: SERIE=0.06909511419485401, SPLIT=Treinamento\n",
            "[REDWINE] Data loaded: n=187, range=[0.00e+00, 1.00e+00], mean=3.47e-01\n",
            "[REDWINE] Splits: Train=99, Val=66, Test=22\n",
            "REDWINE - Run 1/5 - Test MSE: 1.242489e-01, RMSE: 3.524896e-01\n",
            "REDWINE - Run 2/5 - Test MSE: 1.250627e-01, RMSE: 3.536420e-01\n",
            "REDWINE - Run 3/5 - Test MSE: 1.278269e-01, RMSE: 3.575289e-01\n",
            "REDWINE - Run 4/5 - Test MSE: 1.212459e-01, RMSE: 3.482038e-01\n",
            "REDWINE - Run 5/5 - Test MSE: 1.272342e-01, RMSE: 3.566990e-01\n",
            "\n",
            "REDWINE Summary:\n",
            "Test MSE:  1.251237e-01 ± 2.348115e-03\n",
            "Test RMSE: 3.537127e-01 ± 3.326590e-03\n",
            "\n",
            "\n",
            "############################################################\n",
            "DATASET: SUNSPOT\n",
            "############################################################\n",
            "[SUNSPOT] Using columns - SERIE: idx=1 (SERIE), SPLIT: idx=2 (SPLIT)\n",
            "  Row 2: SERIE=0.026288117770767616, SPLIT=Treinamento\n",
            "  Row 3: SERIE=0.05783385909568875, SPLIT=Treinamento\n",
            "  Row 4: SERIE=0.08412197686645637, SPLIT=Treinamento\n",
            "[SUNSPOT] Data loaded: n=288, range=[0.00e+00, 1.00e+00], mean=2.55e-01\n",
            "[SUNSPOT] Splits: Train=156, Val=104, Test=28\n",
            "SUNSPOT - Run 1/5 - Test MSE: 8.743011e-03, RMSE: 9.350407e-02\n",
            "SUNSPOT - Run 2/5 - Test MSE: 8.046298e-03, RMSE: 8.970116e-02\n",
            "SUNSPOT - Run 3/5 - Test MSE: 9.336392e-03, RMSE: 9.662501e-02\n",
            "SUNSPOT - Run 4/5 - Test MSE: 9.272834e-03, RMSE: 9.629556e-02\n",
            "SUNSPOT - Run 5/5 - Test MSE: 1.081977e-02, RMSE: 1.040181e-01\n",
            "\n",
            "SUNSPOT Summary:\n",
            "Test MSE:  9.243661e-03 ± 9.141006e-04\n",
            "Test RMSE: 9.602878e-02 ± 4.704662e-03\n",
            "\n",
            "\n",
            "############################################################\n",
            "DATASET: B1H\n",
            "############################################################\n",
            "[B1H] Using columns - SERIE: idx=1 (SERIE), SPLIT: idx=2 (SPLIT)\n",
            "  Row 2: SERIE=0.4585139980647426, SPLIT=Treinamento\n",
            "  Row 3: SERIE=0.5174402164486508, SPLIT=Treinamento\n",
            "  Row 4: SERIE=0.5721623567002518, SPLIT=Treinamento\n",
            "[B1H] Data loaded: n=1657, range=[0.00e+00, 1.00e+00], mean=2.96e-01\n",
            "[B1H] Splits: Train=814, Val=543, Test=300\n",
            "B1H - Run 1/5 - Test MSE: 5.469038e-04, RMSE: 2.338597e-02\n",
            "B1H - Run 2/5 - Test MSE: 5.269317e-04, RMSE: 2.295499e-02\n",
            "B1H - Run 3/5 - Test MSE: 5.754983e-04, RMSE: 2.398955e-02\n",
            "B1H - Run 4/5 - Test MSE: 5.296733e-04, RMSE: 2.301463e-02\n",
            "B1H - Run 5/5 - Test MSE: 4.766755e-04, RMSE: 2.183290e-02\n",
            "\n",
            "B1H Summary:\n",
            "Test MSE:  5.311365e-04 ± 3.225647e-05\n",
            "Test RMSE: 2.303561e-02 ± 7.051526e-04\n",
            "\n",
            "\n",
            "############################################################\n",
            "ALL EXPERIMENTS COMPLETED!\n",
            "Total successful runs: 50\n",
            "Results saved to: results/lstm_results.csv\n",
            "############################################################\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from openpyxl import load_workbook\n",
        "import time\n",
        "import os\n",
        "from typing import Tuple, Dict\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "DEFAULT_CONFIG = {\n",
        "    \"hidden_size\": 128,\n",
        "    \"num_layers\": 2,\n",
        "    \"dropout\": 0.1,\n",
        "    \"lr\": 0.001,\n",
        "    \"lookback\": 10,\n",
        "}\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "MAX_EPOCHS = 200\n",
        "PATIENCE = 15\n",
        "NUM_RUNS = 5\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# REPRODUCIBILITY\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    \"\"\"Set all random seeds for reproducibility\"\"\"\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "    \"\"\"Seed function for DataLoader workers\"\"\"\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# METRICS\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "def calculate_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
        "    \"\"\"Calculate MSE, RMSE, and R²\"\"\"\n",
        "    mse = np.mean((y_true - y_pred) ** 2)\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
        "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
        "    r2 = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0.0\n",
        "\n",
        "    return {\"mse\": mse, \"rmse\": rmse, \"r2\": r2}\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# DATA LOADING\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "def load_lstm_data_temporal(\n",
        "    dataset_name: str, excel_path: str = \"./datasets/data.xlsx\"\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Load time series data preserving temporal order from Excel\"\"\"\n",
        "\n",
        "    wb = load_workbook(excel_path, read_only=True, data_only=True)\n",
        "\n",
        "    if dataset_name not in wb.sheetnames:\n",
        "        raise ValueError(f\"Dataset '{dataset_name}' not found in workbook\")\n",
        "\n",
        "    ws = wb[dataset_name]\n",
        "\n",
        "    # Read header\n",
        "    header_row = next(ws.iter_rows(min_row=1, max_row=1, values_only=True))\n",
        "    header = [\n",
        "        str(cell).strip().upper() if cell is not None else f\"COL_{i}\"\n",
        "        for i, cell in enumerate(header_row)\n",
        "    ]\n",
        "\n",
        "    # Find SERIE column\n",
        "    serie_col = None\n",
        "    for i, col_name in enumerate(header):\n",
        "        if \"SERIE\" in col_name:\n",
        "            serie_col = i\n",
        "            break\n",
        "\n",
        "    # Find SPLIT column\n",
        "    split_col = None\n",
        "    for i, col_name in enumerate(header):\n",
        "        if \"SPLIT\" in col_name or \"CONJUNTO\" in col_name:\n",
        "            split_col = i\n",
        "            break\n",
        "\n",
        "    if serie_col is None or split_col is None:\n",
        "        raise ValueError(\n",
        "            f\"Could not identify SERIE and SPLIT columns for {dataset_name}\"\n",
        "        )\n",
        "\n",
        "    print(\n",
        "        f\"[{dataset_name}] Using columns - SERIE: idx={serie_col} ({header[serie_col]}), SPLIT: idx={split_col} ({header[split_col]})\"\n",
        "    )\n",
        "\n",
        "    # Read data and show first few rows for verification\n",
        "    data = []\n",
        "    for row_idx, row in enumerate(ws.iter_rows(min_row=2, values_only=True)):\n",
        "        if row[serie_col] is not None:\n",
        "            data.append(row)\n",
        "            # Print first 3 data rows to verify correct columns\n",
        "            if row_idx < 3:\n",
        "                print(\n",
        "                    f\"  Row {row_idx+2}: SERIE={row[serie_col]}, SPLIT={row[split_col]}\"\n",
        "                )\n",
        "\n",
        "    if len(data) == 0:\n",
        "        raise ValueError(f\"No data found in sheet '{dataset_name}'\")\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Extract series and split labels\n",
        "    series_data = pd.to_numeric(df.iloc[:, serie_col], errors=\"coerce\").values\n",
        "    split_labels = df.iloc[:, split_col].astype(str).str.strip().values\n",
        "\n",
        "    # Remove NaN values\n",
        "    valid_mask = ~np.isnan(series_data)\n",
        "    series_data = series_data[valid_mask]\n",
        "    split_labels = split_labels[valid_mask]\n",
        "\n",
        "    # Normalize split labels\n",
        "    split_labels_normalized = []\n",
        "    for label in split_labels:\n",
        "        label_lower = label.lower()\n",
        "        if \"treinamento\" in label_lower:\n",
        "            split_labels_normalized.append(\"Treinamento\")\n",
        "        elif \"valida\" in label_lower:\n",
        "            split_labels_normalized.append(\"Validacao\")\n",
        "        elif \"teste\" in label_lower:\n",
        "            split_labels_normalized.append(\"Teste\")\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown split label: {label}\")\n",
        "\n",
        "    split_labels = np.array(split_labels_normalized)\n",
        "\n",
        "    # Log data statistics for debugging\n",
        "    print(\n",
        "        f\"[{dataset_name}] Data loaded: n={len(series_data)}, range=[{series_data.min():.2e}, {series_data.max():.2e}], mean={series_data.mean():.2e}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"[{dataset_name}] Splits: Train={np.sum(split_labels=='Treinamento')}, Val={np.sum(split_labels=='Validacao')}, Test={np.sum(split_labels=='Teste')}\"\n",
        "    )\n",
        "\n",
        "    # Check for suspicious outliers (values > 10 when data should be in [0,1])\n",
        "    # Remove corrupted values that are clearly data entry errors\n",
        "    outlier_threshold = (\n",
        "        10.0  # If normalized data should be [0,1], anything > 10 is corrupted\n",
        "    )\n",
        "    outlier_mask = series_data > outlier_threshold\n",
        "\n",
        "    if np.any(outlier_mask):\n",
        "        n_outliers = np.sum(outlier_mask)\n",
        "        outlier_indices = np.where(outlier_mask)[0]\n",
        "        print(\n",
        "            f\"[{dataset_name}] WARNING: Found {n_outliers} corrupted values > {outlier_threshold}\"\n",
        "        )\n",
        "        print(f\"  Outlier indices: {outlier_indices[:10]}\")\n",
        "        print(f\"  Outlier values: {series_data[outlier_mask][:10]}\")\n",
        "\n",
        "        # Replace outliers with the median of surrounding valid values\n",
        "        for idx in outlier_indices:\n",
        "            # Get surrounding values (before and after)\n",
        "            before_idx = max(0, idx - 5)\n",
        "            after_idx = min(len(series_data), idx + 6)\n",
        "            surrounding = series_data[before_idx:after_idx]\n",
        "            valid_surrounding = surrounding[surrounding <= outlier_threshold]\n",
        "\n",
        "            if len(valid_surrounding) > 0:\n",
        "                replacement = np.median(valid_surrounding)\n",
        "                series_data[idx] = replacement\n",
        "                print(\n",
        "                    f\"  Replaced index {idx} (value={series_data[idx]:.2e}) with median={replacement:.4f}\"\n",
        "                )\n",
        "            else:\n",
        "                # Fallback: use overall median of series\n",
        "                series_data[idx] = np.median(\n",
        "                    series_data[series_data <= outlier_threshold]\n",
        "                )\n",
        "\n",
        "        print(\n",
        "            f\"[{dataset_name}] Fixed! New range: [{series_data.min():.2e}, {series_data.max():.2e}]\"\n",
        "        )\n",
        "\n",
        "    return series_data, split_labels\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SEQUENCE CREATION\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "def create_sequences_no_leakage(\n",
        "    series_data: np.ndarray, split_labels: np.ndarray, lookback: int\n",
        ") -> Tuple:\n",
        "    \"\"\"\n",
        "    Create sequences without data leakage across splits\n",
        "    - Scaler fits only on training data\n",
        "    - Sequences cannot look back across split boundaries\n",
        "    \"\"\"\n",
        "\n",
        "    # Fit scaler only on training data\n",
        "    train_mask = split_labels == \"Treinamento\"\n",
        "    train_data = series_data[train_mask]\n",
        "\n",
        "    if len(train_data) < 10:\n",
        "        raise ValueError(f\"Insufficient training data: {len(train_data)} points\")\n",
        "\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaler.fit(train_data.reshape(-1, 1))\n",
        "\n",
        "    # Normalize entire series using training statistics\n",
        "    series_normalized = scaler.transform(series_data.reshape(-1, 1)).ravel()\n",
        "\n",
        "    # Create sequences\n",
        "    X_train, y_train, y_train_orig = [], [], []\n",
        "    X_val, y_val, y_val_orig = [], [], []\n",
        "    X_test, y_test, y_test_orig = [], [], []\n",
        "\n",
        "    for i in range(lookback, len(series_data)):\n",
        "        sequence_norm = series_normalized[i - lookback : i]\n",
        "        target_norm = series_normalized[i]\n",
        "        target_orig = series_data[i]\n",
        "        target_split = split_labels[i]\n",
        "\n",
        "        lookback_splits = split_labels[i - lookback : i]\n",
        "\n",
        "        # Only add if entire sequence (lookback + target) is from same split\n",
        "        if target_split == \"Treinamento\" and np.all(lookback_splits == \"Treinamento\"):\n",
        "            X_train.append(sequence_norm)\n",
        "            y_train.append(target_norm)\n",
        "            y_train_orig.append(target_orig)\n",
        "\n",
        "        elif target_split == \"Validacao\" and np.all(lookback_splits == \"Validacao\"):\n",
        "            X_val.append(sequence_norm)\n",
        "            y_val.append(target_norm)\n",
        "            y_val_orig.append(target_orig)\n",
        "\n",
        "        elif target_split == \"Teste\" and np.all(lookback_splits == \"Teste\"):\n",
        "            X_test.append(sequence_norm)\n",
        "            y_test.append(target_norm)\n",
        "            y_test_orig.append(target_orig)\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    X_train = (\n",
        "        np.array(X_train) if len(X_train) > 0 else np.array([]).reshape(0, lookback)\n",
        "    )\n",
        "    y_train = np.array(y_train) if len(y_train) > 0 else np.array([])\n",
        "    y_train_orig = np.array(y_train_orig) if len(y_train_orig) > 0 else np.array([])\n",
        "\n",
        "    X_val = np.array(X_val) if len(X_val) > 0 else np.array([]).reshape(0, lookback)\n",
        "    y_val = np.array(y_val) if len(y_val) > 0 else np.array([])\n",
        "    y_val_orig = np.array(y_val_orig) if len(y_val_orig) > 0 else np.array([])\n",
        "\n",
        "    X_test = np.array(X_test) if len(X_test) > 0 else np.array([]).reshape(0, lookback)\n",
        "    y_test = np.array(y_test) if len(y_test) > 0 else np.array([])\n",
        "    y_test_orig = np.array(y_test_orig) if len(y_test_orig) > 0 else np.array([])\n",
        "\n",
        "    if len(X_train) < 10:\n",
        "        raise ValueError(f\"Insufficient training sequences: {len(X_train)}\")\n",
        "\n",
        "    return (\n",
        "        X_train,\n",
        "        y_train,\n",
        "        X_val,\n",
        "        y_val,\n",
        "        X_test,\n",
        "        y_test,\n",
        "        y_train_orig,\n",
        "        y_val_orig,\n",
        "        y_test_orig,\n",
        "        scaler,\n",
        "    )\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# DATASET CLASS\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for time series\"\"\"\n",
        "\n",
        "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
        "        self.X = torch.FloatTensor(X)\n",
        "        self.y = torch.FloatTensor(y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# LSTM MODEL\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    \"\"\"LSTM for time series forecasting\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size: int = 1,\n",
        "        hidden_size: int = 64,\n",
        "        num_layers: int = 1,\n",
        "        dropout: float = 0.0,\n",
        "    ):\n",
        "        super(LSTMModel, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0.0,\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, (h, c) = self.lstm(x)\n",
        "        out = out[:, -1, :]\n",
        "        out = self.fc(out)\n",
        "        return out.squeeze(-1)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stopping handler\"\"\"\n",
        "\n",
        "    def __init__(self, patience: int = 10, min_delta: float = 0.0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = float(\"inf\")\n",
        "        self.best_state = None\n",
        "        self.best_epoch = 0\n",
        "\n",
        "    def __call__(self, val_loss: float, model: nn.Module, epoch: int) -> bool:\n",
        "        \"\"\"Returns True if should stop training\"\"\"\n",
        "        if val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.best_state = model.state_dict().copy()\n",
        "            self.best_epoch = epoch\n",
        "            self.counter = 0\n",
        "            return False\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            return self.counter >= self.patience\n",
        "\n",
        "    def load_best_state(self, model: nn.Module):\n",
        "        \"\"\"Load best model state\"\"\"\n",
        "        if self.best_state is not None:\n",
        "            model.load_state_dict(self.best_state)\n",
        "\n",
        "\n",
        "def train_one_epoch(\n",
        "    model: nn.Module,\n",
        "    train_loader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        ") -> float:\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(X_batch)\n",
        "        loss = criterion(y_pred, y_batch)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * len(X_batch)\n",
        "\n",
        "    return total_loss / len(train_loader.dataset)\n",
        "\n",
        "\n",
        "def validate_one_epoch(\n",
        "    model: nn.Module, val_loader: DataLoader, criterion: nn.Module\n",
        ") -> float:\n",
        "    \"\"\"Validate for one epoch\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            y_pred = model(X_batch)\n",
        "            loss = criterion(y_pred, y_batch)\n",
        "            total_loss += loss.item() * len(X_batch)\n",
        "\n",
        "    return total_loss / len(val_loader.dataset)\n",
        "\n",
        "\n",
        "def train_lstm(\n",
        "    train_loader: DataLoader,\n",
        "    val_loader: DataLoader,\n",
        "    config: Dict,\n",
        "    max_epochs: int = 200,\n",
        "    patience: int = 15,\n",
        ") -> Tuple[nn.Module, Dict]:\n",
        "    \"\"\"Train LSTM with early stopping\"\"\"\n",
        "    model = LSTMModel(\n",
        "        input_size=1,\n",
        "        hidden_size=config[\"hidden_size\"],\n",
        "        num_layers=config[\"num_layers\"],\n",
        "        dropout=config[\"dropout\"],\n",
        "    ).to(device)\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
        "    early_stopping = EarlyStopping(patience=patience)\n",
        "\n",
        "    history = {\"train_loss\": [], \"val_loss\": [], \"best_epoch\": 0}\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer)\n",
        "        val_loss = validate_one_epoch(model, val_loader, criterion)\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "\n",
        "        if early_stopping(val_loss, model, epoch + 1):\n",
        "            break\n",
        "\n",
        "    early_stopping.load_best_state(model)\n",
        "    history[\"best_epoch\"] = early_stopping.best_epoch\n",
        "\n",
        "    return model, history\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "def evaluate_model(\n",
        "    model: nn.Module, data_loader: DataLoader\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Evaluate model and return predictions\"\"\"\n",
        "    model.eval()\n",
        "    y_true_list, y_pred_list = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in data_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_pred = model(X_batch)\n",
        "\n",
        "            y_true_list.append(y_batch.cpu().numpy())\n",
        "            y_pred_list.append(y_pred.cpu().numpy())\n",
        "\n",
        "    return np.concatenate(y_true_list), np.concatenate(y_pred_list)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# RESULTS SAVING\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "def save_results_to_csv(results_data: Dict, filename: str = \"results/lstm_results.csv\"):\n",
        "    \"\"\"Save results to CSV\"\"\"\n",
        "    os.makedirs(\n",
        "        os.path.dirname(filename) if os.path.dirname(filename) else \".\", exist_ok=True\n",
        "    )\n",
        "\n",
        "    df = pd.DataFrame([results_data])\n",
        "\n",
        "    if os.path.exists(filename):\n",
        "        df.to_csv(filename, mode=\"a\", header=False, index=False)\n",
        "    else:\n",
        "        df.to_csv(filename, mode=\"w\", header=True, index=False)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXPERIMENT PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "def run_single_experiment(\n",
        "    dataset_name: str,\n",
        "    run_num: int,\n",
        "    config: Dict,\n",
        "    series_data: np.ndarray,\n",
        "    split_labels: np.ndarray,\n",
        ") -> Dict:\n",
        "    \"\"\"Run single experiment\"\"\"\n",
        "\n",
        "    set_seed(42 + run_num)\n",
        "    # Always use DEFAULT_CONFIG for the model config\n",
        "    config = DEFAULT_CONFIG\n",
        "    lookback = config[\"lookback\"]\n",
        "\n",
        "    # Create sequences\n",
        "    (\n",
        "        X_train,\n",
        "        y_train,\n",
        "        X_val,\n",
        "        y_val,\n",
        "        X_test,\n",
        "        y_test,\n",
        "        y_train_orig,\n",
        "        y_val_orig,\n",
        "        y_test_orig,\n",
        "        scaler,\n",
        "    ) = create_sequences_no_leakage(series_data, split_labels, lookback)\n",
        "\n",
        "    # Prepare data for LSTM\n",
        "    X_train_reshaped = X_train.reshape(-1, lookback, 1)\n",
        "    X_val_reshaped = X_val.reshape(-1, lookback, 1)\n",
        "    X_test_reshaped = X_test.reshape(-1, lookback, 1)\n",
        "\n",
        "    train_dataset = TimeSeriesDataset(X_train_reshaped, y_train)\n",
        "    val_dataset = TimeSeriesDataset(X_val_reshaped, y_val)\n",
        "    test_dataset = TimeSeriesDataset(X_test_reshaped, y_test)\n",
        "\n",
        "    g = torch.Generator()\n",
        "    g.manual_seed(42 + run_num)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        worker_init_fn=seed_worker,\n",
        "        generator=g,\n",
        "    )\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    train_loader_eval = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    # Train model\n",
        "    start_time = time.time()\n",
        "    model, history = train_lstm(\n",
        "        train_loader, val_loader, config, max_epochs=MAX_EPOCHS, patience=PATIENCE\n",
        "    )\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    # Get predictions and denormalize\n",
        "    _, y_pred_train_norm = evaluate_model(model, train_loader_eval)\n",
        "    _, y_pred_val_norm = evaluate_model(model, val_loader)\n",
        "    _, y_pred_test_norm = evaluate_model(model, test_loader)\n",
        "\n",
        "    y_pred_train = scaler.inverse_transform(y_pred_train_norm.reshape(-1, 1)).ravel()\n",
        "    y_pred_val = scaler.inverse_transform(y_pred_val_norm.reshape(-1, 1)).ravel()\n",
        "    y_pred_test = scaler.inverse_transform(y_pred_test_norm.reshape(-1, 1)).ravel()\n",
        "\n",
        "    # Calculate metrics on original scale\n",
        "    train_metrics = calculate_metrics(y_train_orig, y_pred_train)\n",
        "    val_metrics = calculate_metrics(y_val_orig, y_pred_val)\n",
        "    test_metrics = calculate_metrics(y_test_orig, y_pred_test)\n",
        "\n",
        "    # Prepare results\n",
        "    results = {\n",
        "        \"experiment_num\": run_num,\n",
        "        \"dataset\": dataset_name,\n",
        "        \"method\": \"LSTM\",\n",
        "        \"lookback\": lookback,\n",
        "        \"hidden_size\": config[\"hidden_size\"],\n",
        "        \"num_layers\": config[\"num_layers\"],\n",
        "        \"dropout\": config[\"dropout\"],\n",
        "        \"learning_rate\": config[\"lr\"],\n",
        "        \"best_epoch\": history[\"best_epoch\"],\n",
        "        \"training_time_sec\": training_time,\n",
        "        \"mse_train\": train_metrics[\"mse\"],\n",
        "        \"rmse_train\": train_metrics[\"rmse\"],\n",
        "        \"r2_train\": train_metrics[\"r2\"],\n",
        "        \"mse_val\": val_metrics[\"mse\"],\n",
        "        \"rmse_val\": val_metrics[\"rmse\"],\n",
        "        \"r2_val\": val_metrics[\"r2\"],\n",
        "        \"mse_test\": test_metrics[\"mse\"],\n",
        "        \"rmse_test\": test_metrics[\"rmse\"],\n",
        "        \"r2_test\": test_metrics[\"r2\"],\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def run_dataset_experiments(dataset_name: str):\n",
        "    \"\"\"Run all experiments for a single dataset\"\"\"\n",
        "\n",
        "    # Always use DEFAULT_CONFIG for the model config\n",
        "    config = DEFAULT_CONFIG\n",
        "\n",
        "    # Load data\n",
        "    series_data, split_labels = load_lstm_data_temporal(dataset_name)\n",
        "\n",
        "    # Run multiple experiments\n",
        "    dataset_results = []\n",
        "\n",
        "    for run_num in range(1, NUM_RUNS + 1):\n",
        "        try:\n",
        "            results = run_single_experiment(\n",
        "                dataset_name, run_num, config, series_data, split_labels\n",
        "            )\n",
        "            dataset_results.append(results)\n",
        "            save_results_to_csv(results)\n",
        "            print(\n",
        "                f\"{dataset_name} - Run {run_num}/{NUM_RUNS} - Test MSE: {results['mse_test']:.6e}, RMSE: {results['rmse_test']:.6e}\"\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"{dataset_name} - Run {run_num} failed: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Summary\n",
        "    if dataset_results:\n",
        "        test_mses = [r[\"mse_test\"] for r in dataset_results]\n",
        "        test_rmses = [r[\"rmse_test\"] for r in dataset_results]\n",
        "        print(f\"\\n{dataset_name} Summary:\")\n",
        "        print(f\"Test MSE:  {np.mean(test_mses):.6e} ± {np.std(test_mses):.6e}\")\n",
        "        print(f\"Test RMSE: {np.mean(test_rmses):.6e} ± {np.std(test_rmses):.6e}\\n\")\n",
        "\n",
        "    return dataset_results\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    DATASETS = [\n",
        "        \"CARSALES\",\n",
        "        \"Electricity\",\n",
        "        \"GAS\",\n",
        "        \"LAKEERIE\",\n",
        "        \"Nordic\",\n",
        "        \"PIGS\",\n",
        "        \"POLLUTION\",\n",
        "        \"REDWINE\",\n",
        "        \"SUNSPOT\",\n",
        "        \"B1H\",\n",
        "    ]\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Starting LSTM Experiments\")\n",
        "    print(f\"Datasets: {len(DATASETS)}\")\n",
        "    print(f\"Runs per dataset: {NUM_RUNS}\")\n",
        "    print(f\"Max epochs: {MAX_EPOCHS} (patience={PATIENCE})\")\n",
        "    print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    for dataset_name in DATASETS:\n",
        "        print(f\"\\n{'#'*60}\")\n",
        "        print(f\"DATASET: {dataset_name}\")\n",
        "        print(f\"{'#'*60}\")\n",
        "\n",
        "        results = run_dataset_experiments(dataset_name)\n",
        "        all_results.extend(results)\n",
        "\n",
        "    print(f\"\\n{'#'*60}\")\n",
        "    print(f\"ALL EXPERIMENTS COMPLETED!\")\n",
        "    print(f\"Total successful runs: {len(all_results)}\")\n",
        "    print(f\"Results saved to: results/lstm_results.csv\")\n",
        "    print(f\"{'#'*60}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
